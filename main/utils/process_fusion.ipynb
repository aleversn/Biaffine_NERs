{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计合成数据的label types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_repair\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_label(files_list=[]):\n",
    "    labels_dict = {}\n",
    "    for file in files_list:\n",
    "        with open(file, encoding='utf-8', mode='r') as f:\n",
    "            ori_list = f.readlines()\n",
    "        try:\n",
    "            for idx, item in enumerate(tqdm(ori_list)):\n",
    "                item = item.split('\\t')\n",
    "                json_item = json_repair.loads(item[1])\n",
    "                for item in json_item:\n",
    "                    if 'type' in item:\n",
    "                        if item['type'] not in labels_dict:\n",
    "                            labels_dict[item['type']] = 1\n",
    "                    if 'pos' in item:\n",
    "                        if item['pos'] not in labels_dict:\n",
    "                            labels_dict[item['pos']] = 1\n",
    "        except:\n",
    "            print(item)\n",
    "            print(len(item[0]))\n",
    "            json_repair.loads(item[1])\n",
    "    return labels_dict\n",
    "\n",
    "labels_dict = collect_label(files_list=['/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/entity_train_1350.json', '/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/entity_train_1350.json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取entity和pos的label_dict并进行校正处理,获得label_format函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_repair\n",
    "from tqdm import tqdm\n",
    "\n",
    "label_dict = {\n",
    "    'OTHER': ['OTHER', 'other']\n",
    "}\n",
    "\n",
    "\n",
    "def update_label_dict(file_name):\n",
    "    with open(file_name) as f:\n",
    "        e_dict = json_repair.load(f)\n",
    "    for key in e_dict:\n",
    "        cur = e_dict[key]\n",
    "        if key not in cur:\n",
    "            cur.append(key)\n",
    "        if key not in label_dict:\n",
    "            label_dict[key] = cur\n",
    "        else:\n",
    "            label_dict[key] += cur\n",
    "\n",
    "update_label_dict('/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/entity_label.json')\n",
    "update_label_dict('/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/pos_label.json')\n",
    "\n",
    "for key in label_dict:\n",
    "    cur = label_dict[key]\n",
    "    new_cur = []\n",
    "    for item in cur:\n",
    "        if item not in new_cur:\n",
    "            new_cur.append(item)\n",
    "    label_dict[key] = new_cur\n",
    "\n",
    "label_format_dict = {}\n",
    "\n",
    "for key in label_dict:\n",
    "    for key_item in label_dict[key]:\n",
    "        label_format_dict[key_item] = key\n",
    "\n",
    "def label_format(key):\n",
    "    if key in label_format_dict:\n",
    "        return label_format_dict[key]\n",
    "    return 'WORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_format('OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/lpc/repos/CNNNER/datasets/few_shot/resume/train_1350.jsonl') as f:\n",
    "    ori_data = f.readlines()\n",
    "ori_data = [json_repair.loads(i) for i in ori_data]\n",
    "\n",
    "dataset_fusion_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/entity_train_1350.json', encoding='utf-8', mode='r') as f:\n",
    "    ori_list = f.readlines()\n",
    "for idx, item in enumerate(tqdm(ori_list)):\n",
    "    item = item.split('\\t')\n",
    "    json_item = json_repair.loads(item[1])\n",
    "    exists_2d = {}\n",
    "    for item in json_item:\n",
    "        if 'entity' not in item or 'type' not in item: continue\n",
    "        entity, entity_type = str(item['entity']), item['type']\n",
    "        ent_len = len(entity)\n",
    "        if ent_len <= 1:\n",
    "            continue\n",
    "        entity_type = label_format(entity_type)\n",
    "        if entity_type not in dataset_fusion_labels:\n",
    "            dataset_fusion_labels.append(entity_type)\n",
    "        ori_text_list = ori_data[idx]['text']\n",
    "        for i in range(len(ori_text_list) - ent_len + 1):\n",
    "            if ''.join(ori_text_list[i:i+ent_len]) == entity:\n",
    "                if i not in exists_2d:\n",
    "                    exists_2d[i] = {}\n",
    "                    if (i + ent_len) not in exists_2d[i]:\n",
    "                        ori_data[idx]['entities'].append({'start': i, 'end': i+ent_len, 'entity': entity_type, 'text': ori_text_list[i:i+ent_len]})\n",
    "                        exists_2d[i][i+ent_len] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/pos_train_1350.json', encoding='utf-8', mode='r') as f:\n",
    "    ori_list = f.readlines()\n",
    "for idx, item in enumerate(tqdm(ori_list)):\n",
    "    item = item.split('\\t')\n",
    "    json_item = json_repair.loads(item[1])\n",
    "    exists_2d = {}\n",
    "    for item in json_item:\n",
    "        if 'word' not in item or 'pos' not in item: continue\n",
    "        entity, entity_type = str(item['word']), item['pos']\n",
    "        ent_len = len(entity)\n",
    "        if ent_len <= 1:\n",
    "            continue\n",
    "        entity_type = label_format(entity_type)\n",
    "        if entity_type not in dataset_fusion_labels:\n",
    "            dataset_fusion_labels.append(entity_type)\n",
    "        ori_text_list = ori_data[idx]['text']\n",
    "        for i in range(len(ori_text_list) - ent_len + 1):\n",
    "            if ''.join(ori_text_list[i:i+ent_len]) == entity:\n",
    "                if i not in exists_2d:\n",
    "                    exists_2d[i] = {}\n",
    "                    if (i + ent_len) not in exists_2d[i]:\n",
    "                        ori_data[idx]['entities'].append({'start': i, 'end': i+ent_len, 'entity': entity_type, 'text': ori_text_list[i:i+ent_len]})\n",
    "                        # ori_data[idx]['entities'].append({'start': i, 'end': i+ent_len, 'entity': 'WORD', 'text': ori_text_list[i:i+ent_len]})\n",
    "                        exists_2d[i][i+ent_len] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/train_1350_fusion.jsonl', 'w') as f:\n",
    "    for item in tqdm(ori_data):\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/lpc/repos/CNNNER/datasets/few_shot/resume_DA/labels_fusion.txt', 'w') as f:\n",
    "    for label in dataset_fusion_labels:\n",
    "        f.write(label + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnnner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
